package com.twitter.analysis;

import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.apache.spark.SparkConf;
import org.apache.spark.streaming.Durations;
import org.apache.spark.streaming.api.java.*;
import org.apache.spark.streaming.kafka010.*;

// Importations HDFS n√©cessaires
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import java.io.BufferedWriter;
import java.io.OutputStreamWriter;

import java.util.*;

public class TwitterSparkConsumers {
    public static void main(String[] args) throws InterruptedException {
    	System.out.println("üö¶ [Spark] En attente de tweets depuis Kafka...");

        // ‚úÖ Configuration Spark pour le streaming local
        SparkConf conf = new SparkConf()
                .setMaster("local[*]")
                .setAppName("TwitterSparkConsumers");

        // ‚úÖ Cr√©ation du contexte de streaming (batch toutes les 5 secondes)
        JavaStreamingContext streamingContext = new JavaStreamingContext(conf, Durations.seconds(5));

        // ‚úÖ Param√®tres Kafka
        Map<String, Object> kafkaParams = new HashMap<>();
        kafkaParams.put("bootstrap.servers", "localhost:9092");
        kafkaParams.put("key.deserializer", StringDeserializer.class);
        kafkaParams.put("value.deserializer", StringDeserializer.class);
        kafkaParams.put("group.id", "tweet-consumer-group");
        kafkaParams.put("auto.offset.reset", "earliest"); // üîÅ Lire depuis le d√©but
        kafkaParams.put("enable.auto.commit", false);

        // ‚úÖ Nom du topic Kafka (doit correspondre au Producer)
        Collection<String> topics = Arrays.asList("tweets-topic");

        // ‚úÖ Cr√©ation du flux de donn√©es depuis Kafka
        JavaInputDStream<ConsumerRecord<String, String>> stream =
                KafkaUtils.createDirectStream(
                        streamingContext,
                        LocationStrategies.PreferConsistent(),
                        ConsumerStrategies.Subscribe(topics, kafkaParams)
                );

        // ‚úÖ Traitement et affichage des tweets re√ßus
        // Int√©gration HDFS dans foreachRDD
        stream.map(ConsumerRecord::value)
              .foreachRDD(rdd -> {
                  List<String> tweets = rdd.collect(); // Collecte tous les tweets du RDD en une liste
                  if (tweets.isEmpty()) {
                      System.out.println("Aucun tweet re√ßu dans ce batch.");
                  } else {
                      System.out.println(" [Spark] Traitement de " + tweets.size() + " tweets re√ßus.");

                      // --- D√©but de l'int√©gration HDFS ---
                      // Cr√©ation de la configuration Hadoop
                      // Assurez-vous que le NameNode HDFS est accessible via localhost:9000
                      Configuration hadoopConf = new Configuration();
                      hadoopConf.set("fs.defaultFS", "hdfs://localhost:9000"); //

                      // Chemin de sortie HDFS. Utilisez un timestamp pour un nom de fichier unique par batch.
                      String timestamp = new java.text.SimpleDateFormat("yyyyMMddHHmmss").format(new Date());
                      Path outputPath = new Path("/user/hp/tweets_raw_data/batch_" + timestamp + ".txt"); //

                      try (FileSystem hdfs = FileSystem.get(hadoopConf); // Obtient une instance du syst√®me de fichiers HDFS
                           BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(hdfs.create(outputPath, true)))) { // Cr√©e le fichier dans HDFS, 'true' pour √©craser s'il existe
                          
                          // Cr√©ation des r√©pertoires parents si ils n'existent pas
                          Path parentDir = outputPath.getParent();
                          if (!hdfs.exists(parentDir)) {
                              hdfs.mkdirs(parentDir); // Cr√©e tous les r√©pertoires parents n√©cessaires
                              System.out.println("Cr√©ation du r√©pertoire HDFS : " + parentDir);
                          }

                          for (String tweet : tweets) {
                              System.out.println(" Tweet re√ßu : " + tweet); // Affiche le tweet dans la console Spark
                              writer.write(tweet); // √âcrit le tweet dans le fichier HDFS
                              writer.newLine(); // Ajoute une nouvelle ligne apr√®s chaque tweet
                          }
                          System.out.println("üíæ [HDFS] " + tweets.size() + " tweets √©crits dans HDFS : " + outputPath); //

                      } catch (Exception e) {
                          System.err.println(" Erreur lors de l'√©criture dans HDFS : " + e.getMessage()); //
                          e.printStackTrace();
                      }
                      // --- Fin de l'int√©gration HDFS ---

                      System.out.println("‚úÖ [Spark] Fin de traitement du batch.\n");
                  }
              });

        // ‚úÖ Lancement du traitement en continu
        streamingContext.start();
        streamingContext.awaitTermination();
    }
}